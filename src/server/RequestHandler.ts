/**
 * ğŸš€ é©å‘½æ€§è¯·æ±‚å¤„ç†å™¨
 * âœ¨ å®Œå…¨é‡å†™ï¼Œæ— ç¡¬ç¼–ç é™åˆ¶ï¼
 * ğŸ¨ å®Œæ•´çš„å¤šæ¨¡æ€ã€å‡½æ•°è°ƒç”¨å’ŒåŠ¨æ€æ¨¡å‹æ”¯æŒ
 */

import * as http from 'http';
import * as vscode from 'vscode';

import { logger } from '../utils/Logger';
import { Converter } from '../utils/Converter';
import { Validator, ValidationError } from '../utils/Validator';
import { ModelDiscoveryService } from '../services/ModelDiscoveryService';
import { FunctionCallService } from '../services/FunctionCallService';

import {
    ModelCapabilities,
    EnhancedMessage,
    EnhancedRequestContext
} from '../types/ModelCapabilities';
import { OpenAITool, ValidatedRequest } from '../types/OpenAI';

import { ServerState } from '../types/VSCode';
import { 
    LIMITS,
    HTTP_STATUS, 
    CONTENT_TYPES, 
    SSE_HEADERS,
    ERROR_CODES,
    NOTIFICATIONS
} from '../constants/Config';

export class RequestHandler {
    private modelDiscovery: ModelDiscoveryService;
    private functionService: FunctionCallService;
    private isInitialized: boolean = false;
    
    constructor() {
        this.modelDiscovery = new ModelDiscoveryService();
        this.functionService = new FunctionCallService();
        
        // å¼‚æ­¥åˆå§‹åŒ–
        this.initialize();
    }
    
    /**
     * ğŸš€ åˆå§‹åŒ–å¤„ç†å™¨
     */
    private async initialize(): Promise<void> {
        try {
            logger.info('ğŸš€ Initializing Enhanced Request Handler...');
            
            // å‘ç°æ‰€æœ‰å¯ç”¨æ¨¡å‹
            await this.modelDiscovery.discoverAllModels();
            
            this.isInitialized = true;
            logger.info('âœ… Enhanced Request Handler initialized successfully!');
            
        } catch (error) {
            logger.error('âŒ Failed to initialize Enhanced Request Handler:', error as Error);
        }
    }
    
    /**
     * ğŸ¨ å¤„ç†èŠå¤©å®Œæˆï¼Œå…·å¤‡å®Œæ•´å¤šæ¨¡æ€æ”¯æŒ
     */
    public async handleChatCompletions(
        req: http.IncomingMessage,
        res: http.ServerResponse,
        requestId: string
    ): Promise<void> {
        const requestLogger = logger.createRequestLogger(requestId);
        const startTime = Date.now();
        
        try {
            // ç¡®ä¿æˆ‘ä»¬å·²åˆå§‹åŒ–
            if (!this.isInitialized) {
                await this.initialize();
            }
            
            requestLogger.info('ğŸš€ Processing enhanced chat completion request');
            
            // è¯»å–å¹¶è§£æè¯·æ±‚ä½“å¹¶éªŒè¯
            let body: string;
            try {
                body = await this.readRequestBody(req);
            } catch (bodyReadError) {
                if (bodyReadError instanceof Error && bodyReadError.message === 'Request body too large') {
                    requestLogger.warn('Request body exceeded configured limit');
                    this.sendErrorResponse(
                        res,
                        HTTP_STATUS.PAYLOAD_TOO_LARGE,
                        'Request body too large',
                        ERROR_CODES.INVALID_REQUEST,
                        requestId
                    );
                    if (!req.destroyed) {
                        req.resume();
                        req.once('end', () => {
                            if (!req.destroyed) {
                                req.destroy();
                            }
                        });
                    }
                    return;
                }

                if (bodyReadError instanceof Error && bodyReadError.message === 'Request aborted by client') {
                    requestLogger.warn('Client aborted request while reading body');
                    return;
                }

                throw bodyReadError;
            }
            
            // è§£æJSONå¹¶å¤„ç†é”™è¯¯
            let rawRequestData: any;
            try {
                rawRequestData = JSON.parse(body);
            } catch (parseError) {
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.BAD_REQUEST,
                    'Invalid JSON in request body',
                    ERROR_CODES.INVALID_REQUEST,
                    requestId
                );
                return;
            }
            
            // ä½¿ç”¨éªŒè¯å™¨éªŒè¯è¯·æ±‚
            let validatedRequest: ValidatedRequest;
            try {
                validatedRequest = Validator.validateChatCompletionRequest(
                    rawRequestData,
                    this.modelDiscovery.getAllModels()
                );
            } catch (validationError) {
                if (validationError instanceof ValidationError) {
                    this.sendErrorResponse(
                        res,
                        HTTP_STATUS.BAD_REQUEST,
                        validationError.message,
                        validationError.code,
                        requestId,
                        validationError.param
                    );
                } else {
                    this.sendErrorResponse(
                        res,
                        HTTP_STATUS.BAD_REQUEST,
                        'Request validation failed',
                        ERROR_CODES.INVALID_REQUEST,
                        requestId
                    );
                }
                return;
            }
            
            const requestData: ValidatedRequest = validatedRequest;
            
            // æå–å¢å¼ºæ¶ˆæ¯å’Œè¯·æ±‚å‚æ•°
            const messages: EnhancedMessage[] = requestData.messages;
            const requestedModel = requestData.model;
            const isStream = requestData.stream || false;
            const functions = requestData.functions || [];
            const tools: OpenAITool[] = requestData.tools || [];
            const toolChoice = requestData.tool_choice;
            const functionCall = requestData.function_call;
            const preferLegacyFunctionCall = functions.length > 0 && tools.length === 0;
            const requiresToolCall = this.isRequiredToolMode(toolChoice, functionCall);
            const requiredModeParam = toolChoice !== undefined ? 'tool_choice' : 'function_call';
            
            requestLogger.info('ğŸ“‹ Request analysis:', {
                model: requestedModel,
                stream: isStream,
                messageCount: messages.length,
                hasImages: messages.some(m => Array.isArray(m.content) && 
                    m.content.some(p => p.type === 'image_url')),
                hasFunctions: functions.length > 0 || tools.length > 0
            });
            
            // åˆ›å»ºå¢å¼ºä¸Šä¸‹æ–‡
            const context = Converter.createEnhancedContext(
                requestId,
                requestedModel,
                isStream,
                messages,
                undefined, // Will be set after model selection
                this.getClientIP(req),
                req.headers['user-agent']
            );
            
            // ğŸ¯ ä»…å…è®¸ç›´æ¥ä½¿ç”¨è¯·æ±‚çš„æ¨¡å‹ï¼ˆå®Œå…¨ç§»é™¤è‡ªåŠ¨é€‰æ‹©ï¼‰
            let selectedModel: ModelCapabilities | null = this.modelDiscovery.getModel(requestedModel) || null;
            if (!selectedModel) {
                // å¦‚æœæ‰¾ä¸åˆ°æ¨¡å‹ï¼Œå°è¯•é‡æ–°å‘ç°æ¨¡å‹
                await this.modelDiscovery.discoverAllModels();
                selectedModel = this.modelDiscovery.getModel(requestedModel) || null;
            }
            
            if (!selectedModel) {
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.SERVICE_UNAVAILABLE,
                    `Requested model '${requestedModel}' not found or unavailable`,
                    ERROR_CODES.API_ERROR,
                    requestId
                );
                return;
            }
            
            // ç”¨æ‰€é€‰æ¨¡å‹æ›´æ–°ä¸Šä¸‹æ–‡
            context.selectedModel = selectedModel;
            
            // å§‹ç»ˆ directï¼šæ—¥å¿—æ¸…æ™°è¡¨æ˜ä½¿ç”¨è¯·æ±‚çš„æ¨¡å‹
            requestLogger.info('âœ… Model direct:', {
                model: requestedModel,
                vendor: selectedModel.vendor,
                family: selectedModel.family,
                maxTokens: selectedModel.maxInputTokens,
                supportsVision: selectedModel.supportsVision,
                supportsTools: selectedModel.supportsTools
            });
            
            // æ£€æŸ¥ Copilot è®¿é—®æƒé™
            const hasAccess = await this.checkCopilotAccess();
            if (!hasAccess) {
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.UNAUTHORIZED,
                    NOTIFICATIONS.NO_COPILOT_ACCESS,
                    ERROR_CODES.AUTHENTICATION_ERROR,
                    requestId
                );
                return;
            }
            
            // éªŒè¯ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼ˆåŠ¨æ€ï¼ï¼‰
            if (context.estimatedTokens > selectedModel.maxInputTokens) {
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.BAD_REQUEST,
                    `Request exceeds model context limit (${context.estimatedTokens} > ${selectedModel.maxInputTokens} tokens)`,
                    ERROR_CODES.INVALID_REQUEST,
                    requestId
                );
                return;
            }
            
            // å°†æ¶ˆæ¯è½¬æ¢ä¸º VS Code æ ¼å¼
            const vsCodeMessages = await Converter.convertMessagesToVSCode(
                messages, 
                selectedModel
            );
            
            // å¦‚æœè¯·æ±‚åŒ…å«å·¥å…·å®šä¹‰ï¼Œåˆ™å‡†å¤‡ VS Code å·¥å…·é…ç½®
            let vsCodeTools: vscode.LanguageModelChatTool[] = [];
            let toolMode: vscode.LanguageModelChatToolMode | undefined;
            if (functions.length > 0 || tools.length > 0) {
                try {
                    const toolConfig = this.functionService.prepareToolsForRequest(
                        functions,
                        tools,
                        toolChoice,
                        functionCall
                    );
                    vsCodeTools = toolConfig.tools;
                    toolMode = toolConfig.toolMode;
                    requestLogger.info(`ğŸ› ï¸ Prepared ${vsCodeTools.length} tools for LM request`, {
                        toolMode,
                        requestedTools: tools.length,
                        requestedFunctions: functions.length
                    });
                } catch (error) {
                    requestLogger.warn('Failed to prepare tools:', { error: String(error) });
                    if (requiresToolCall) {
                        this.sendErrorResponse(
                            res,
                            HTTP_STATUS.BAD_REQUEST,
                            `Failed to prepare required tools: ${this.stringifyError(error)}`,
                            ERROR_CODES.INVALID_REQUEST,
                            requestId,
                            requiredModeParam
                        );
                        return;
                    }
                }
            }

            if (!selectedModel.supportsTools && vsCodeTools.length > 0) {
                requestLogger.warn('Model capability probe reports supportsTools=false, continuing with runtime attempt', {
                    model: selectedModel.id,
                    requiredToolMode: requiresToolCall,
                    tools: vsCodeTools.length
                });
            }
            
            // ğŸš€ å‘ VS CODE LM API å‘é€è¯·æ±‚
            try {
                requestLogger.info('ğŸ“¨ Sending request to VS Code LM API...');
                
                const requestOptions: vscode.LanguageModelChatRequestOptions = {};
                if (vsCodeTools.length > 0) {
                    requestOptions.tools = vsCodeTools;
                }
                if (toolMode && vsCodeTools.length > 0) {
                    requestOptions.toolMode = toolMode;
                }
                const requestCancellation = new vscode.CancellationTokenSource();
                const cancelOnAborted = () => {
                    if (!requestCancellation.token.isCancellationRequested) {
                        requestLogger.warn('Client request aborted, cancelling LM request');
                        requestCancellation.cancel();
                    }
                };
                const cancelOnClose = () => {
                    if (!res.writableEnded && !requestCancellation.token.isCancellationRequested) {
                        requestLogger.warn('Client connection closed, cancelling LM request');
                        requestCancellation.cancel();
                    }
                };

                req.once('aborted', cancelOnAborted);
                res.once('close', cancelOnClose);

                try {
                    const response = await this.sendRequestWithToolFallback(
                        selectedModel.vsCodeModel,
                        vsCodeMessages,
                        requestOptions,
                        requestCancellation.token,
                        requestLogger,
                        vsCodeTools.length > 0 && !requiresToolCall
                    );
                    
                    // ğŸŒŠ å¤„ç†æµå¼ä¸éæµå¼å“åº”
                    if (isStream) {
                        await this.handleStreamingResponse(
                            response,
                            res,
                            context,
                            requestLogger,
                            requiresToolCall,
                            requiredModeParam
                        );
                    } else {
                        await this.handleNonStreamingResponse(
                            response,
                            res,
                            context,
                            requestLogger,
                            preferLegacyFunctionCall,
                            requiresToolCall,
                            requiredModeParam
                        );
                    }
                } finally {
                    req.removeListener('aborted', cancelOnAborted);
                    res.removeListener('close', cancelOnClose);
                    requestCancellation.dispose();
                }
                
            } catch (lmError) {
                requestLogger.error('âŒ VS Code LM API error:', lmError as Error);

                if (requiresToolCall && vsCodeTools.length > 0 && this.isLikelyToolModeError(lmError)) {
                    this.sendErrorResponse(
                        res,
                        HTTP_STATUS.BAD_REQUEST,
                        `Model failed to satisfy required tool-calling request: ${this.stringifyError(lmError)}`,
                        ERROR_CODES.INVALID_REQUEST,
                        requestId,
                        toolChoice !== undefined ? 'tool_choice' : 'function_call'
                    );
                    return;
                }
                
                // ä½¿ç”¨å¢å¼ºé”™è¯¯æ˜ å°„å¤„ç†ç‰¹å®šçš„ LM API é”™è¯¯
                if (lmError instanceof vscode.LanguageModelError) {
                    this.handleLanguageModelError(lmError, res, requestId);
                } else {
                    this.sendErrorResponse(
                        res,
                        HTTP_STATUS.BAD_GATEWAY,
                        `Language model request failed: ${lmError}`,
                        ERROR_CODES.API_ERROR,
                        requestId
                    );
                }
            }
            
        } catch (error) {
            const duration = Date.now() - startTime;
            requestLogger.error(`âŒ Request failed after ${duration}ms:`, error as Error);
            
            if (!res.headersSent) {
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.INTERNAL_SERVER_ERROR,
                    'Enhanced request processing failed',
                    ERROR_CODES.API_ERROR,
                    requestId
                );
            }
        }
    }
    
    /**
     * ğŸ“‹ å¤„ç†å¢å¼ºæ¨¡å‹ç«¯ç‚¹
     */
    public async handleModels(
        req: http.IncomingMessage,
        res: http.ServerResponse,
        requestId: string
    ): Promise<void> {
        const requestLogger = logger.createRequestLogger(requestId);
        
        try {
            requestLogger.info('ğŸ“‹ Fetching all available models (no limitations!)...');
            
            // ç¡®ä¿æˆ‘ä»¬å·²åˆå§‹åŒ–
            if (!this.isInitialized) {
                await this.initialize();
            }
            
            // è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
            const allModels = this.modelDiscovery.getAllModels();
            
            requestLogger.info(`ğŸ“Š Found ${allModels.length} total models:`);
            
            // ä¸ºé€æ˜åº¦è®°å½•æ¨¡å‹èƒ½åŠ›
            allModels.forEach(model => {
                requestLogger.info(`  âœ¨ ${model.id}: tokens=${model.maxInputTokens}, vision=${model.supportsVision}, tools=${model.supportsTools}`);
            });
            
            const modelsResponse = Converter.createModelsResponse(allModels);
            
            res.writeHead(HTTP_STATUS.OK, { 'Content-Type': CONTENT_TYPES.JSON });
            res.end(JSON.stringify(modelsResponse, null, 2));
            
            requestLogger.info(`âœ… Models response sent with ${modelsResponse.data.length} models`);
            
        } catch (error) {
            requestLogger.error('âŒ Error handling models request:', error as Error);
            this.sendErrorResponse(
                res,
                HTTP_STATUS.INTERNAL_SERVER_ERROR,
                'Failed to retrieve models',
                ERROR_CODES.API_ERROR,
                requestId
            );
        }
    }
    
    /**
     * ğŸ‘©â€âš•ï¸ å¢å¼ºå¥åº·æ£€æŸ¥
     */
    public async handleHealth(
        req: http.IncomingMessage,
        res: http.ServerResponse,
        requestId: string,
        serverState: ServerState
    ): Promise<void> {
        try {
            const modelPool = this.modelDiscovery.getModelPool();
            const healthResponse = Converter.createHealthResponse(serverState, modelPool);
            
            res.writeHead(HTTP_STATUS.OK, { 'Content-Type': CONTENT_TYPES.JSON });
            res.end(JSON.stringify(healthResponse, null, 2));
            
        } catch (error) {
            this.sendErrorResponse(
                res,
                HTTP_STATUS.INTERNAL_SERVER_ERROR,
                'Health check failed',
                ERROR_CODES.API_ERROR,
                requestId
            );
        }
    }
    
    /**
     * ğŸ“‹ å¢å¼ºçŠ¶æ€ç«¯ç‚¹
     */
    public async handleStatus(
        req: http.IncomingMessage,
        res: http.ServerResponse,
        requestId: string,
        serverState: ServerState
    ): Promise<void> {
        try {
            const modelPool = this.modelDiscovery.getModelPool();
            const toolStats = this.functionService.getToolStats();
            
            const status = {
                server: serverState,
                models: {
                    total: modelPool.primary.length + modelPool.secondary.length + modelPool.fallback.length,
                    primary: modelPool.primary.length,
                    secondary: modelPool.secondary.length,
                    fallback: modelPool.fallback.length,
                    unhealthy: modelPool.unhealthy.length,
                    lastUpdated: modelPool.lastUpdated.toISOString(),
                    capabilities: {
                        vision: modelPool.primary.filter(m => m.supportsVision).length,
                        tools: modelPool.primary.filter(m => m.supportsTools).length,
                        multimodal: modelPool.primary.filter(m => m.supportsMultimodal).length
                    }
                },
                tools: {
                    available: Object.keys(toolStats).length,
                    stats: toolStats
                },
                features: {
                    dynamicModelDiscovery: true,
                    multimodalSupport: true,
                    functionCalling: true,
                    noHardcodedLimitations: true,
                    autoModelSelection: true,
                    loadBalancing: true
                },
                copilot: {
                    available: await this.checkCopilotAccess(),
                    models: await this.getModelCount()
                },
                timestamp: new Date().toISOString()
            };
            
            res.writeHead(HTTP_STATUS.OK, { 'Content-Type': CONTENT_TYPES.JSON });
            res.end(JSON.stringify(status, null, 2));
            
        } catch (error) {
            this.sendErrorResponse(
                res,
                HTTP_STATUS.INTERNAL_SERVER_ERROR,
                'Status check failed',
                ERROR_CODES.API_ERROR,
                requestId
            );
        }
    }
    
    /**
     * ğŸŒŠ å¤„ç†å¢å¼ºæµå¼å“åº”
     */
    private async handleStreamingResponse(
        response: vscode.LanguageModelChatResponse,
        res: http.ServerResponse,
        context: EnhancedRequestContext,
        requestLogger: any,
        requiresToolCall: boolean,
        requiredModeParam: string
    ): Promise<void> {
        try {
            requestLogger.info('ğŸŒŠ Starting enhanced streaming response...');
            
            let chunkCount = 0;
            
            for await (const chunk of Converter.extractStreamContent(
                response, 
                context, 
                context.selectedModel!,
                {
                    requiresToolCall
                }
            )) {
                if (!res.headersSent) {
                    res.writeHead(HTTP_STATUS.OK, SSE_HEADERS);
                }
                res.write(chunk);
                chunkCount++;
            }
            
            requestLogger.info(`âœ… Enhanced streaming completed: ${chunkCount} chunks sent`);
            
        } catch (error) {
            requestLogger.error('âŒ Enhanced streaming error:', error);
            const errorText = this.stringifyError(error);
            if (requiresToolCall && errorText.includes('required tool mode')) {
                if (!res.headersSent) {
                    this.sendErrorResponse(
                        res,
                        HTTP_STATUS.BAD_REQUEST,
                        'Model did not produce any tool calls despite required mode',
                        ERROR_CODES.INVALID_REQUEST,
                        context.requestId,
                        requiredModeParam
                    );
                    return;
                }

                const errorEvent = Converter.createSSEEvent('error', {
                    message: 'Model did not produce any tool calls despite required mode',
                    type: ERROR_CODES.INVALID_REQUEST,
                    param: requiredModeParam
                });
                res.write(errorEvent);
                return;
            }

            if (!res.headersSent) {
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.BAD_GATEWAY,
                    'Enhanced stream processing error',
                    ERROR_CODES.API_ERROR,
                    context.requestId
                );
                return;
            }
            
            const errorEvent = Converter.createSSEEvent('error', {
                message: 'Enhanced stream processing error',
                type: ERROR_CODES.API_ERROR
            });
            res.write(errorEvent);
        } finally {
            if (!res.writableEnded) {
                res.end();
            }
        }
    }
    
    /**
     * ğŸ“‹ å¤„ç†å¢å¼ºéæµå¼å“åº”
     */
    private async handleNonStreamingResponse(
        response: vscode.LanguageModelChatResponse,
        res: http.ServerResponse,
        context: EnhancedRequestContext,
        requestLogger: any,
        preferLegacyFunctionCall: boolean,
        requiresToolCall: boolean,
        requiredModeParam: string
    ): Promise<void> {
        try {
            requestLogger.info('ğŸ“‹ Collecting enhanced full response...');
            
            const fullResponse = await Converter.collectFullResponse(response);
            if (requiresToolCall && fullResponse.toolCalls.length === 0) {
                requestLogger.warn('Model returned no tool calls despite required mode');
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.BAD_REQUEST,
                    'Model did not produce any tool calls despite required mode',
                    ERROR_CODES.INVALID_REQUEST,
                    context.requestId,
                    requiredModeParam
                );
                return;
            }
            
            const completionResponse = Converter.createCompletionResponse(
                fullResponse.content, 
                context,
                context.selectedModel!,
                fullResponse.toolCalls,
                preferLegacyFunctionCall
            );
            
            res.writeHead(HTTP_STATUS.OK, { 'Content-Type': CONTENT_TYPES.JSON });
            res.end(JSON.stringify(completionResponse, null, 2));
            
            requestLogger.info('âœ… Enhanced response sent:', {
                contentLength: fullResponse.content.length,
                toolCalls: fullResponse.toolCalls.length,
                tokens: completionResponse.usage.total_tokens,
                model: context.selectedModel!.id
            });
            
        } catch (error) {
            requestLogger.error('âŒ Error collecting enhanced response:', error as Error);
            throw error;
        }
    }

    /**
     * ğŸ§­ æ˜¯å¦ä¸ºå¼ºåˆ¶å·¥å…·è°ƒç”¨è¯­ä¹‰ï¼ˆä¸èƒ½è‡ªåŠ¨é™çº§ï¼‰
     */
    private isRequiredToolMode(
        toolChoice: ValidatedRequest['tool_choice'],
        functionCall: ValidatedRequest['function_call']
    ): boolean {
        if (toolChoice === 'required') {
            return true;
        }
        if (toolChoice && typeof toolChoice === 'object') {
            return true;
        }
        if (functionCall && typeof functionCall === 'object') {
            return true;
        }
        return false;
    }

    /**
     * ğŸš¦ å‘é€è¯·æ±‚ï¼šå·¥å…·å¤±è´¥æ—¶å¯é€‰è‡ªåŠ¨é™çº§é‡è¯•ä¸€æ¬¡
     */
    private async sendRequestWithToolFallback(
        model: vscode.LanguageModelChat,
        messages: vscode.LanguageModelChatMessage[],
        requestOptions: vscode.LanguageModelChatRequestOptions,
        token: vscode.CancellationToken,
        requestLogger: any,
        allowToolFallback: boolean
    ): Promise<vscode.LanguageModelChatResponse> {
        try {
            return await model.sendRequest(messages, requestOptions, token);
        } catch (error) {
            const hasTools = Array.isArray(requestOptions.tools) && requestOptions.tools.length > 0;
            if (
                !allowToolFallback ||
                !hasTools ||
                token.isCancellationRequested ||
                !this.isLikelyToolModeError(error)
            ) {
                throw error;
            }

            requestLogger.warn('LM request with tools failed due probable tool-mode incompatibility, retrying once without tools', {
                error: String(error)
            });

            const fallbackOptions: vscode.LanguageModelChatRequestOptions = {
                ...requestOptions,
                tools: undefined,
                toolMode: undefined
            };

            try {
                return await model.sendRequest(messages, fallbackOptions, token);
            } catch (fallbackError) {
                requestLogger.error('LM fallback request without tools also failed', fallbackError as Error);
                throw fallbackError;
            }
        }
    }

    /**
     * ğŸ” åˆ¤æ–­æ˜¯å¦æ˜¯å·¥å…·æ¨¡å¼ç›¸å…³é”™è¯¯ï¼ˆç”¨äºæ§åˆ¶æ˜¯å¦é™çº§ï¼‰
     */
    private isLikelyToolModeError(error: unknown): boolean {
        const message = this.stringifyError(error).toLowerCase();
        return (
            message.includes('toolmode') ||
            message.includes('tool mode') ||
            message.includes('tool_mode') ||
            message.includes('tools are not supported') ||
            message.includes('does not support tools') ||
            message.includes('does not support function') ||
            message.includes('function call is not supported') ||
            /\btool\s*(call|use|invocation)\b/.test(message)
        );
    }

    /**
     * ğŸ§¾ ç»Ÿä¸€é”™è¯¯æ–‡æœ¬æå–
     */
    private stringifyError(error: unknown): string {
        if (error instanceof Error) {
            return error.message || String(error);
        }
        return String(error);
    }
    
    /**
     * ğŸ”® æ£€æŸ¥ Copilot è®¿é—®æƒé™
     */
    private async checkCopilotAccess(): Promise<boolean> {
        try {
            const models = await vscode.lm.selectChatModels({ vendor: 'copilot' });
            return models.length > 0;
        } catch (error) {
            logger.warn('Copilot access check failed:', { error: String(error) });
            return false;
        }
    }
    
    /**
     * ğŸ“‹ è·å–æ¨¡å‹æ€»æ•°
     */
    private async getModelCount(): Promise<number> {
        try {
            const models = await vscode.lm.selectChatModels();
            return models.length;
        } catch (error) {
            return 0;
        }
    }
    
    /**
     * âŒ å¤„ç† VS Code è¯­è¨€æ¨¡å‹ç‰¹å®šé”™è¯¯
     */
    private handleLanguageModelError(
        error: vscode.LanguageModelError,
        res: http.ServerResponse,
        requestId: string
    ): void {
        let statusCode: number = HTTP_STATUS.INTERNAL_SERVER_ERROR;
        let errorCode: string = ERROR_CODES.API_ERROR;
        let message = error.message;
        
        // å¢å¼ºé”™è¯¯æ˜ å°„
        switch (error.code) {
            case 'NoPermissions':
                statusCode = HTTP_STATUS.FORBIDDEN;
                errorCode = ERROR_CODES.PERMISSION_ERROR;
                message = 'Permission denied for language model access';
                break;
            case 'Blocked':
                statusCode = HTTP_STATUS.FORBIDDEN;
                errorCode = ERROR_CODES.PERMISSION_ERROR;
                message = 'Request blocked by content filter';
                break;
            case 'NotFound':
                statusCode = HTTP_STATUS.NOT_FOUND;
                errorCode = ERROR_CODES.NOT_FOUND_ERROR;
                message = 'Language model not found';
                break;
            case 'ContextLengthExceeded':
                statusCode = HTTP_STATUS.BAD_REQUEST;
                errorCode = ERROR_CODES.INVALID_REQUEST;
                message = 'Request exceeds context length limit';
                break;
            default:
                statusCode = HTTP_STATUS.BAD_GATEWAY;
                errorCode = ERROR_CODES.API_ERROR;
                message = `Language model error: ${error.message}`;
        }
        
        this.sendErrorResponse(res, statusCode, message, errorCode, requestId);
    }
    
    /**
     * âŒ å‘é€å¢å¼ºé”™è¯¯å“åº”
     */
    private sendErrorResponse(
        res: http.ServerResponse,
        statusCode: number,
        message: string,
        type: string,
        requestId: string,
        param?: string
    ): void {
        if (res.headersSent) {
            return;
        }
        
        const errorResponse = Converter.createErrorResponse(message, type, undefined, param);

        const headers: Record<string, string> = { 'Content-Type': CONTENT_TYPES.JSON };
        if (statusCode === HTTP_STATUS.PAYLOAD_TOO_LARGE) {
            headers.Connection = 'close';
        }

        res.writeHead(statusCode, headers);
        res.end(JSON.stringify(errorResponse, null, 2));
        
        logger.error(`âŒ Enhanced error response: ${statusCode}`, new Error(message), { type, param }, requestId);
    }
    
    /**
     * ğŸ“‹ è¯»å–è¯·æ±‚ä½“
     */
    private async readRequestBody(req: http.IncomingMessage): Promise<string> {
        return new Promise((resolve, reject) => {
            const chunks: Buffer[] = [];
            let totalBytes = 0;
            let settled = false;

            const cleanup = () => {
                req.removeListener('data', onData);
                req.removeListener('end', onEnd);
                req.removeListener('error', onError);
                req.removeListener('aborted', onAborted);
            };

            const fail = (error: Error) => {
                if (settled) {
                    return;
                }
                settled = true;
                cleanup();
                reject(error);
            };

            const onData = (chunk: Buffer | string) => {
                const chunkBuffer = typeof chunk === 'string' ? Buffer.from(chunk) : chunk;
                totalBytes += chunkBuffer.length;

                if (totalBytes > LIMITS.MAX_REQUEST_BODY_BYTES) {
                    req.resume();
                    req.once('end', () => {
                        if (!req.destroyed) {
                            req.destroy();
                        }
                    });
                    fail(new Error('Request body too large'));
                    return;
                }

                chunks.push(chunkBuffer);
            };

            const onEnd = () => {
                if (settled) {
                    return;
                }
                settled = true;
                cleanup();
                resolve(Buffer.concat(chunks).toString('utf8'));
            };

            const onError = (error: Error) => fail(error);
            const onAborted = () => fail(new Error('Request aborted by client'));

            req.on('data', onData);
            req.once('end', onEnd);
            req.once('error', onError);
            req.once('aborted', onAborted);
        });
    }
    
    /**
     * ğŸ“ è·å–å®¢æˆ·ç«¯IPåœ°å€
     */
    private getClientIP(req: http.IncomingMessage): string {
        return (req.headers['x-forwarded-for'] as string)?.split(',')[0] ||
               req.socket?.remoteAddress ||
               '127.0.0.1';
    }
    
    /**
     * ğŸ§¹ æ¸…ç†èµ„æº
     */
    public dispose(): void {
        this.modelDiscovery.dispose();
        this.functionService.dispose();
    }
}
